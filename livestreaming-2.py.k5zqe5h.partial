#!/usr/bin/env python
# coding: utf-8

# In[1]:


from imutils.video import VideoStream
import numpy as np
import argparse
import imutils
import time
import cv2
import os
import tensorflow as tf
from model import select_model, get_checkpoint


# In[9]:


tf.app.flags.DEFINE_string('model_dir', '',
                           'Model directory (where training data lives)')

tf.app.flags.DEFINE_string('class_type', 'age',
                           'Classification type (age|gender)')


tf.app.flags.DEFINE_string('device_id', '/cpu:0',
                           'What processing unit to execute inference on')

tf.app.flags.DEFINE_string('filename', '',
                           'File (Image) or File list (Text/No header TSV) to process')

tf.app.flags.DEFINE_string('target', '',
                           'CSV file containing the filename processed along with best guess and score')

tf.app.flags.DEFINE_string('checkpoint', 'checkpoint',
                          'Checkpoint basename')

tf.app.flags.DEFINE_string('model_type', 'default',
                           'Type of convnet')

tf.app.flags.DEFINE_string('requested_step', '', 'Within the model directory, a requested step to restore e.g., 9000')

tf.app.flags.DEFINE_boolean('single_look', False, 'single look at the image or multiple crops')

tf.app.flags.DEFINE_string('face_detection_model', '', 'Do frontal face detection with model specified')

tf.app.flags.DEFINE_string('face_detection_type', 'cascade', 'Face detection model type (yolo_tiny|cascade)')

FLAGS = tf.app.flags.FLAGS


# In[2]:


RESIZE_FINAL = 227
GENDER_LIST =['M','F']
AGE_LIST = ['(18,19)','(20,24)','(25,29)','(30,34)','(35,39)']
MAX_BATCH_SZ = 128


# In[3]:


def getFaceBox(net, frame, conf_threshold=0.7):
    frameOpencvDnn = frame.copy()
    frameHeight = frameOpencvDnn.shape[0]
    frameWidth = frameOpencvDnn.shape[1]
    blob = cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)

    net.setInput(blob)
    detections = net.forward()
    bboxes = []
    for i in range(detections.shape[2]):
        confidence = detections[0, 0, i, 2]
        if confidence > conf_threshold:
            x1 = int(detections[0, 0, i, 3] * frameWidth)
            y1 = int(detections[0, 0, i, 4] * frameHeight)
            x2 = int(detections[0, 0, i, 5] * frameWidth)
            y2 = int(detections[0, 0, i, 6] * frameHeight)
            bboxes.append([x1, y1, x2, y2])
            cv2.rectangle(frameOpencvDnn, (x1, y1), (x2, y2), (0, 255, 0), int(round(frameHeight/150)), 8)
    return frameOpencvDnn, bboxes


# In[4]:


def classify_many_single_crop(sess, label_list, softmax_output, coder, images, image_files, writer):
    try:
        num_batches = math.ceil(len(image_files) / MAX_BATCH_SZ)
        pg = ProgressBar(num_batches)
        for j in range(num_batches):
            start_offset = j * MAX_BATCH_SZ
            end_offset = min((j + 1) * MAX_BATCH_SZ, len(image_files))
            
            batch_image_files = image_files[start_offset:end_offset]
            print(start_offset, end_offset, len(batch_image_files))
            image_batch = make_multi_image_batch(batch_image_files, coder)
            batch_results = sess.run(softmax_output, feed_dict={images:image_batch.eval()})
            batch_sz = batch_results.shape[0]
            for i in range(batch_sz):
                output_i = batch_results[i]
                best_i = np.argmax(output_i)
                best_choice = (label_list[best_i], output_i[best_i])
                print('Guess @ 1 %s, prob = %.2f' % best_choice)
                if writer is not None:
                    f = batch_image_files[i]
                    writer.writerow((f, best_choice[0], '%.2f' % best_choice[1]))
            pg.update()
        pg.done()
    except Exception as e:
        print(e)
        print('Failed to run all images')


# In[5]:


faceProto = "opencv_face_detector.pbtxt"
faceModel = "opencv_face_detector_uint8.pb"
MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)


# In[6]:


# FACE RECOGNITION MODEL
faceNet = cv2.dnn.readNet(faceModel,faceProto)
config = tf.ConfigProto(allow_soft_placement=True)

# AGE MODEL
with tf.Session(config=config) as sess:
    
    age_lable_list = AGE_LIST 
    ageNet = select_model('defualt')


# In[7]:


# Open a video file or an image file or a camera stream
cap = cv2.VideoCapture(0)
padding = 20


# In[12]:


while cv2.waitKey(1) < 0:
    # Read frame
    t = time.time()
    hasFrame, frame = cap.read()
    if not hasFrame:
        cv2.waitKey()
        break
    frameFace, bboxes = getFaceBox(faceNet, frame)
    if not bboxes:
        print("No face Detected, Checking next frame")
        continue

    for bbox in bboxes:
        # print(bbox)
        face = frame[max(0,bbox[1]-padding):min(bbox[3]+padding,frame.shape[0]-1),max(0,bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]
        
        with tf.device('/cpu:0'):

            images = tf.placeholder(tf.float32, [None, RESIZE_FINAL, RESIZE_FINAL, 3])
            
            #logits = ageNet(len(AGE_LIST), images, 1, False)
            init = tf.global_variables_initializer()

            #requested_step = FLAGS.requested_step if FLAGS.requested_step else None
            requested_step = 14999

            #checkpoint_path = '%s' % (FLAGS.model_dir)
            checkpoint_path = '%s' % ('C:/Users/LEE/Desktop/rude-carnie/age_model')

            #print(checkpoint_path)
            model_checkpoint_path, global_step = get_checkpoint(checkpoint_path, requested_step, FLAGS.checkpoint)
            print(model_checkpoint_path)
            saver = tf.train.Saver()
            saver.restore(sess, model_checkpoint_path)

            softmax_output = tf.nn.softmax(logits)

            coder = ImageCoder()
        
        writer = None
        output = None
        
        classify_many_single_crop(sess, AGE_LIST, softmax_output, coder, images, image_files, writer)

        
        #blob = cv.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)
        #genderNet.setInput(blob)
        #genderPreds = genderNet.forward()
        #gender = genderList[genderPreds[0].argmax()]
        
        #print("Gender : {}, confidence = {:.3f}".format(gender, genderPreds[0].max()))

        #ageNet.setInput(blob)
        #agePreds = ageNet.forward()
        #age = ageList[agePreds[0].argmax()]
        
        #print("Age : {}, confidence = {:.3f}".format(age, agePreds[0].max()))

        label = "{},{}".format('M', '20')
        cv2.putText(frameFace, label, (bbox[0]-5, bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0,255), 2, cv2.LINE_AA)
        cv2.imshow("Age Gender Demo", frameFace)
        #name = args.i
        #cv.imwrite('./detected/'+name,frameFace)
    print("Time : {:.3f}".format(time.time() - t))


# In[ ]:




