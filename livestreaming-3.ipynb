{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\yolo\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda3\\envs\\yolo\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda3\\envs\\yolo\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda3\\envs\\yolo\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda3\\envs\\yolo\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda3\\envs\\yolo\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from model import select_model, get_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_string('model_dir', '',\n",
    "                           'Model directory (where training data lives)')\n",
    "\n",
    "tf.app.flags.DEFINE_string('class_type', 'age',\n",
    "                           'Classification type (age|gender)')\n",
    "\n",
    "\n",
    "tf.app.flags.DEFINE_string('device_id', '/cpu:0',\n",
    "                           'What processing unit to execute inference on')\n",
    "\n",
    "tf.app.flags.DEFINE_string('filename', '',\n",
    "                           'File (Image) or File list (Text/No header TSV) to process')\n",
    "\n",
    "tf.app.flags.DEFINE_string('target', '',\n",
    "                           'CSV file containing the filename processed along with best guess and score')\n",
    "\n",
    "tf.app.flags.DEFINE_string('checkpoint', 'checkpoint',\n",
    "                          'Checkpoint basename')\n",
    "\n",
    "tf.app.flags.DEFINE_string('model_type', 'default',\n",
    "                           'Type of convnet')\n",
    "\n",
    "tf.app.flags.DEFINE_string('requested_step', '', 'Within the model directory, a requested step to restore e.g., 9000')\n",
    "\n",
    "tf.app.flags.DEFINE_boolean('single_look', False, 'single look at the image or multiple crops')\n",
    "\n",
    "tf.app.flags.DEFINE_string('face_detection_model', '', 'Do frontal face detection with model specified')\n",
    "\n",
    "tf.app.flags.DEFINE_string('face_detection_type', 'cascade', 'Face detection model type (yolo_tiny|cascade)')\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESIZE_FINAL = 227\n",
    "GENDER_LIST =['M','F']\n",
    "AGE_LIST = ['(18,19)','(20,24)','(25,29)','(30,34)','(35,39)']\n",
    "MAX_BATCH_SZ = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFaceBox(net, frame, conf_threshold=0.7):\n",
    "    frameOpencvDnn = frame.copy()\n",
    "    frameHeight = frameOpencvDnn.shape[0]\n",
    "    frameWidth = frameOpencvDnn.shape[1]\n",
    "    blob = cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    bboxes = []\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > conf_threshold:\n",
    "            x1 = int(detections[0, 0, i, 3] * frameWidth)\n",
    "            y1 = int(detections[0, 0, i, 4] * frameHeight)\n",
    "            x2 = int(detections[0, 0, i, 5] * frameWidth)\n",
    "            y2 = int(detections[0, 0, i, 6] * frameHeight)\n",
    "            bboxes.append([x1, y1, x2, y2])\n",
    "            cv2.rectangle(frameOpencvDnn, (x1, y1), (x2, y2), (0, 255, 0), int(round(frameHeight/150)), 8)\n",
    "    return frameOpencvDnn, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_one_multi_crop(sess, label_list, softmax_output, coder, images, image, writer):\n",
    "    try:\n",
    "        # 속도 개선\n",
    "        image_batch = make_multi_crop_batch(image, coder)\n",
    "        print('size of batches', image_batch.shape)\n",
    "        print('session :', sess)\n",
    "        batch_results = sess.run(softmax_output, feed_dict={images:image_batch.eval(session=sess)})\n",
    "        output = batch_results[0]\n",
    "        batch_sz = batch_results.shape[0]\n",
    "        for i in range(1, batch_sz):\n",
    "            output = output + batch_results[i]\n",
    "        \n",
    "        output /= batch_sz\n",
    "        best = np.argmax(output)\n",
    "        best_choice = (label_list[best], output[best])\n",
    "        print('Guess @ 1 %s, prob = %.2f' % best_choice)\n",
    "    \n",
    "        if writer is not None:\n",
    "            writer.writerow((best_choice[0], '%.2f' % best_choice[1]))\n",
    "        return best_choice\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Failed to run image')\n",
    "        return '인식', '안됨'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceProto = \"opencv_face_detector.pbtxt\"\n",
    "faceModel = \"opencv_face_detector_uint8.pb\"\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACE RECOGNITION MODEL\n",
    "faceNet = cv2.dnn.readNet(faceModel,faceProto)\n",
    "coder = ImageCoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected default model\n",
      "C:/Users/LEE/Desktop/rude-carnie/age_model/checkpoint-14999\n"
     ]
    }
   ],
   "source": [
    "# AGE MODEL\n",
    "age_graph = tf.Graph()\n",
    "with age_graph.as_default():\n",
    "    images = tf.placeholder(tf.float32, [None, RESIZE_FINAL, RESIZE_FINAL, 3])\n",
    "    ageNet = select_model('default')\n",
    "    logits = ageNet(len(AGE_LIST), images, 1, False)\n",
    "    requested_step = 14999\n",
    "    checkpoint_path = '%s' % ('C:/Users/LEE/Desktop/rude-carnie/age_model')\n",
    "    model_checkpoint_path, global_step = get_checkpoint(checkpoint_path, requested_step, 'checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected default model\n",
      "C:/Users/LEE/Desktop/rude-carnie/gender_model/checkpoint-29999\n"
     ]
    }
   ],
   "source": [
    "# GENDER MODEL\n",
    "gender_graph = tf.Graph()\n",
    "with gender_graph.as_default():\n",
    "    images2 = tf.placeholder(tf.float32, [None, RESIZE_FINAL, RESIZE_FINAL, 3])\n",
    "    genderNet = select_model('default')\n",
    "    logits2  = genderNet(len(GENDER_LIST), images2, 1, False)\n",
    "    requested_step2 = 29999\n",
    "    checkpoint_path2 = '%s' % ('C:/Users/LEE/Desktop/rude-carnie/gender_model')\n",
    "    model_checkpoint_path2, global_step = get_checkpoint(checkpoint_path2, requested_step2, 'checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'output/output:0' shape=(?, 2) dtype=float32>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_sess = tf.Session(graph=age_graph) \n",
    "gender_sess = tf.Session(graph=gender_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/Users/LEE/Desktop/rude-carnie/age_model/checkpoint-14999\n",
      "INFO:tensorflow:Restoring parameters from C:/Users/LEE/Desktop/rude-carnie/gender_model/checkpoint-29999\n"
     ]
    }
   ],
   "source": [
    "with age_sess.as_default():\n",
    "    with age_graph.as_default():\n",
    "        tf.global_variables_initializer().run()\n",
    "        age_saver = tf.train.Saver(tf.global_variables())\n",
    "        age_saver.restore(age_sess, model_checkpoint_path)\n",
    "\n",
    "with gender_sess.as_default():\n",
    "    with gender_graph.as_default():\n",
    "        tf.global_variables_initializer().run()\n",
    "        gender_saver = tf.train.Saver(tf.global_variables())\n",
    "        gender_saver.restore(gender_sess, model_checkpoint_path2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x1b2cf8c8cf8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x1b2c2fb2550>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sess = tf.Session()\\n\\n# AGE MODEL\\nimages = tf.placeholder(tf.float32, [None, RESIZE_FINAL, RESIZE_FINAL, 3])\\nageNet = select_model('default')\\nlogits = ageNet(len(AGE_LIST), images, 1, False)\\nrequested_step = 14999\\ncheckpoint_path = '%s' % ('C:/Users/LEE/Desktop/rude-carnie/age_model')\\nmodel_checkpoint_path, global_step = get_checkpoint(checkpoint_path, requested_step, 'checkpoint')\\n\\nage_saver = tf.train.Saver()\\nage_saver.restore(sess, model_checkpoint_path)\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''sess = tf.Session()\n",
    "\n",
    "# AGE MODEL\n",
    "images = tf.placeholder(tf.float32, [None, RESIZE_FINAL, RESIZE_FINAL, 3])\n",
    "ageNet = select_model('default')\n",
    "logits = ageNet(len(AGE_LIST), images, 1, False)\n",
    "requested_step = 14999\n",
    "checkpoint_path = '%s' % ('C:/Users/LEE/Desktop/rude-carnie/age_model')\n",
    "model_checkpoint_path, global_step = get_checkpoint(checkpoint_path, requested_step, 'checkpoint')\n",
    "\n",
    "age_saver = tf.train.Saver()\n",
    "age_saver.restore(sess, model_checkpoint_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a video file or an image file or a camera stream\n",
    "cap = cv2.VideoCapture(1)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,800)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT,800)\n",
    "padding = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_output2 = tf.nn.softmax(logits2)\n",
    "softmax_output = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running multi-cropped image\n",
      "size of batches (12, 227, 227, 3)\n",
      "session : <tensorflow.python.client.session.Session object at 0x000001B2CF8C8CF8>\n",
      "Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph.\n",
      "Failed to run image\n",
      "Time : 0.639\n",
      "Running multi-cropped image\n",
      "size of batches (12, 227, 227, 3)\n",
      "session : <tensorflow.python.client.session.Session object at 0x000001B2CF8C8CF8>\n",
      "Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph.\n",
      "Failed to run image\n",
      "Time : 0.605\n",
      "Running multi-cropped image\n",
      "size of batches (12, 227, 227, 3)\n",
      "session : <tensorflow.python.client.session.Session object at 0x000001B2CF8C8CF8>\n",
      "Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph.\n",
      "Failed to run image\n",
      "Time : 0.478\n",
      "Running multi-cropped image\n",
      "size of batches (12, 227, 227, 3)\n",
      "session : <tensorflow.python.client.session.Session object at 0x000001B2CF8C8CF8>\n",
      "Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph.\n",
      "Failed to run image\n",
      "Time : 0.458\n",
      "Running multi-cropped image\n",
      "size of batches (12, 227, 227, 3)\n",
      "session : <tensorflow.python.client.session.Session object at 0x000001B2CF8C8CF8>\n",
      "Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph.\n",
      "Failed to run image\n",
      "Time : 0.502\n",
      "Running multi-cropped image\n",
      "size of batches (12, 227, 227, 3)\n",
      "session : <tensorflow.python.client.session.Session object at 0x000001B2CF8C8CF8>\n",
      "Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph.\n",
      "Failed to run image\n",
      "Time : 0.467\n",
      "Running multi-cropped image\n",
      "size of batches (12, 227, 227, 3)\n",
      "session : <tensorflow.python.client.session.Session object at 0x000001B2CF8C8CF8>\n",
      "Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph.\n",
      "Failed to run image\n",
      "Time : 0.482\n",
      "Running multi-cropped image\n",
      "size of batches (12, 227, 227, 3)\n",
      "session : <tensorflow.python.client.session.Session object at 0x000001B2CF8C8CF8>\n",
      "Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph.\n",
      "Failed to run image\n",
      "Time : 0.457\n",
      "Running multi-cropped image\n",
      "size of batches (12, 227, 227, 3)\n",
      "session : <tensorflow.python.client.session.Session object at 0x000001B2CF8C8CF8>\n",
      "Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph.\n",
      "Failed to run image\n",
      "Time : 0.471\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.3.0) C:\\projects\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 1080000 bytes in function 'cv::OutOfMemoryError'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-e4d546428005>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mframeFace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetFaceBox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfaceNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-ded3255d011a>\u001b[0m in \u001b[0;36mgetFaceBox\u001b[1;34m(net, frame, conf_threshold)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mframeHeight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframeOpencvDnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mframeWidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframeOpencvDnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mblob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblobFromImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframeOpencvDnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m104\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m117\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m123\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.3.0) C:\\projects\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 1080000 bytes in function 'cv::OutOfMemoryError'\n"
     ]
    }
   ],
   "source": [
    "while 1:\n",
    "    # Read frame\n",
    "    t = time.time()\n",
    "    hasFrame, frame = cap.read()\n",
    "    if not hasFrame:\n",
    "        cv2.waitKey()\n",
    "        break\n",
    "    frameFace, bboxes = getFaceBox(faceNet, frame)\n",
    "    \n",
    "    if not bboxes:\n",
    "        print(\"No face Detected, Checking next frame\")\n",
    "        continue\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        face = frame[max(0,bbox[1]-padding):min(bbox[3]+padding,frame.shape[0]-1),max(0,bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n",
    "        wit\n",
    "        h tf.device('/cpu:0'):\n",
    "            writer = None\n",
    "            output = None\n",
    "            \n",
    "            \n",
    "            age,prob = classify_one_multi_crop(age_sess, AGE_LIST, softmax_output, coder, images, face, writer)\n",
    "            #gender,prob = classify_one_multi_crop(gender_sess, GENDER_LIST, softmax_output2, coder, images2, face, writer)\n",
    "            \n",
    "            # 이 부분에서 이 에러가 납니다.\n",
    "            # Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph.\n",
    "            \n",
    "            gender = ''\n",
    "        label = \"{} {}\".format(gender,age)\n",
    "        cv2.putText(frameFace, label, (bbox[0]-5, bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0,255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow(\"Age Gender Demo\", frameFace)\n",
    "    \n",
    "    print(\"Time : {:.3f}\".format(time.time() - t))\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YOLO",
   "language": "python",
   "name": "yolo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
