{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from model import select_model, get_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_string('model_dir', '',\n",
    "                           'Model directory (where training data lives)')\n",
    "\n",
    "tf.app.flags.DEFINE_string('class_type', 'age',\n",
    "                           'Classification type (age|gender)')\n",
    "\n",
    "\n",
    "tf.app.flags.DEFINE_string('device_id', '/cpu:0',\n",
    "                           'What processing unit to execute inference on')\n",
    "\n",
    "tf.app.flags.DEFINE_string('filename', '',\n",
    "                           'File (Image) or File list (Text/No header TSV) to process')\n",
    "\n",
    "tf.app.flags.DEFINE_string('target', '',\n",
    "                           'CSV file containing the filename processed along with best guess and score')\n",
    "\n",
    "tf.app.flags.DEFINE_string('checkpoint', 'checkpoint',\n",
    "                          'Checkpoint basename')\n",
    "\n",
    "tf.app.flags.DEFINE_string('model_type', 'default',\n",
    "                           'Type of convnet')\n",
    "\n",
    "tf.app.flags.DEFINE_string('requested_step', '', 'Within the model directory, a requested step to restore e.g., 9000')\n",
    "\n",
    "tf.app.flags.DEFINE_boolean('single_look', False, 'single look at the image or multiple crops')\n",
    "\n",
    "tf.app.flags.DEFINE_string('face_detection_model', '', 'Do frontal face detection with model specified')\n",
    "\n",
    "tf.app.flags.DEFINE_string('face_detection_type', 'cascade', 'Face detection model type (yolo_tiny|cascade)')\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESIZE_FINAL = 227\n",
    "GENDER_LIST =['M','F']\n",
    "AGE_LIST = ['(18,19)','(20,24)','(25,29)','(30,34)','(35,39)']\n",
    "MAX_BATCH_SZ = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFaceBox(net, frame, conf_threshold=0.7):\n",
    "    frameOpencvDnn = frame.copy()\n",
    "    frameHeight = frameOpencvDnn.shape[0]\n",
    "    frameWidth = frameOpencvDnn.shape[1]\n",
    "    blob = cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    bboxes = []\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > conf_threshold:\n",
    "            x1 = int(detections[0, 0, i, 3] * frameWidth)\n",
    "            y1 = int(detections[0, 0, i, 4] * frameHeight)\n",
    "            x2 = int(detections[0, 0, i, 5] * frameWidth)\n",
    "            y2 = int(detections[0, 0, i, 6] * frameHeight)\n",
    "            bboxes.append([x1, y1, x2, y2])\n",
    "            cv2.rectangle(frameOpencvDnn, (x1, y1), (x2, y2), (0, 255, 0), int(round(frameHeight/150)), 8)\n",
    "    return frameOpencvDnn, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_one_multi_crop(sess, label_list, softmax_output, coder, images, image, writer):\n",
    "    try:\n",
    "\n",
    "        image_batch = make_multi_crop_batch(image, coder)\n",
    "        print('size of batches', image_batch.shape)\n",
    "        \n",
    "        batch_results = sess.run(softmax_output, feed_dict={images:image_batch.eval(session=sess)})\n",
    "        output = batch_results[0]\n",
    "        batch_sz = batch_results.shape[0]\n",
    "        for i in range(1, batch_sz):\n",
    "            output = output + batch_results[i]\n",
    "        \n",
    "        output /= batch_sz\n",
    "        best = np.argmax(output)\n",
    "        best_choice = (label_list[best], output[best])\n",
    "        print('Guess @ 1 %s, prob = %.2f' % best_choice)\n",
    "    \n",
    "        if writer is not None:\n",
    "            writer.writerow((best_choice[0], '%.2f' % best_choice[1]))\n",
    "        return best_choice\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Failed to run image')\n",
    "        return '인식', '안됨'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceProto = \"opencv_face_detector.pbtxt\"\n",
    "faceModel = \"opencv_face_detector_uint8.pb\"\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACE RECOGNITION MODEL\n",
    "faceNet = cv2.dnn.readNet(faceModel,faceProto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "\n",
    "sess = tf.Session()    \n",
    "images = tf.placeholder(tf.float32, [None, RESIZE_FINAL, RESIZE_FINAL, 3])\n",
    "\n",
    "# AGE MODEL\n",
    "ageNet = select_model('default')\n",
    "logits = ageNet(len(AGE_LIST), images, 1, False)\n",
    "images2 = tf.placeholder(tf.float32, [None, RESIZE_FINAL, RESIZE_FINAL, 3])\n",
    "\n",
    "requested_step = 14999\n",
    "\n",
    "'''\n",
    "# GENDER MODEL\n",
    "genderNet = select_model('bn')\n",
    "\n",
    "logits2 = genderNet(len(GENDER_LIST), images2, 1, False)\n",
    "\n",
    "#init = tf.global_variables_initializer()\n",
    "#requested_step = FLAGS.requested_step if FLAGS.requested_step else None\n",
    "\n",
    "requested_step2 = 29999\n",
    "\n",
    "#checkpoint_path = '%s' % (FLAGS.model_dir)\n",
    "'''\n",
    "\n",
    "checkpoint_path = '%s' % ('C:/Users/LEE/Desktop/rude-carnie/age_model')\n",
    "#checkpoint_path2 = '%s' % ('C:/Users/LEE/Desktop/rude-carnie/gender_model')\n",
    "\n",
    "model_checkpoint_path, global_step = get_checkpoint(checkpoint_path, requested_step, 'checkpoint')\n",
    "#model_checkpoint_path2, global_step = get_checkpoint(checkpoint_path2, requested_step2, 'checkpoint')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "#saver2 = tf.train.Saver()\n",
    "saver.restore(sess, model_checkpoint_path)\n",
    "#saver2.restore(sess2, model_checkpoint_path2)\n",
    "coder = ImageCoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a video file or an image file or a camera stream\n",
    "cap = cv2.VideoCapture(1)\n",
    "padding = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "while 1:\n",
    "    # Read frame\n",
    "    t = time.time()\n",
    "    hasFrame, frame = cap.read()\n",
    "    if not hasFrame:\n",
    "        cv2.waitKey()\n",
    "        break\n",
    "    frameFace, bboxes = getFaceBox(faceNet, frame)\n",
    "    \n",
    "    if not bboxes:\n",
    "        print(\"No face Detected, Checking next frame\")\n",
    "        continue\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        # print(bbox)\n",
    "        face = frame[max(0,bbox[1]-padding):min(bbox[3]+padding,frame.shape[0]-1),max(0,bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n",
    "        with tf.device('/cpu:0'):\n",
    "            #softmax_output = tf.nn.softmax(logits)\n",
    "            #softmax_output2 = tf.nn.softmax(logits2)\n",
    "\n",
    "            writer = None\n",
    "            output = None\n",
    "        \n",
    "            #age,prob = classify_one_multi_crop(sess, AGE_LIST, softmax_output, coder, images, face, writer)\n",
    "            #gender,prob = age,prob = classify_one_multi_crop(sess, GENDER_LIST, softmax_output2, coder, images2, face, writer)\n",
    "             \n",
    "        label = \"{}\".format('123')\n",
    "        cv2.putText(frameFace, label, (bbox[0]-5, bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0,255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow(\"Age Gender Demo\", frameFace)\n",
    "    \n",
    "    print(\"Time : {:.3f}\".format(time.time() - t))\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YOLO",
   "language": "python",
   "name": "yolo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
